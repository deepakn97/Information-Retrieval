{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import nltk \n",
    "from time import time\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk import FreqDist,ngrams,word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/deepak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/deepak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/deepak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/deepak/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    stop_words = Counter(stopwords.words('english'))\n",
    "    ans = []\n",
    "    for each in data:\n",
    "        if(each not in stop_words.keys()):\n",
    "            ans.append(each)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(tokens):\n",
    "    filtered_document = []\n",
    "    wordnet_pos = ''\n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for i in pos_tokens:\n",
    "        if i[1].startswith('J'):\n",
    "            wordnet_pos = wordnet.ADJ\n",
    "        elif i[1].startswith('V'):\n",
    "            wordnet_pos = wordnet.VERB\n",
    "        elif i[1].startswith('N'):\n",
    "            wordnet_pos = wordnet.NOUN\n",
    "        elif i[1].startswith('R'):\n",
    "            wordnet_pos = wordnet.ADV\n",
    "        else:\n",
    "            wordnet_pos = wordnet.ADJ_SAT\n",
    "        filtered_document.append(lmtzr.lemmatize(i[0],pos = wordnet_pos))\n",
    "    return filtered_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapterwise(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatizer(tokens)\n",
    "        \n",
    "    fDist = FreqDist(tokens)\n",
    "    dictionary_freq = dict(zip(fDist.keys(),fDist.values()))\n",
    "    sorted_dict_freq = sorted(dictionary_freq.items(), key=lambda x: x[1],reverse= True)\n",
    "    \n",
    "    return dict(sorted_dict_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book(dataset, top_k = 20, num_chap = 10, ch = False):\n",
    "    data = pd.read_csv(dataset)\n",
    "    \n",
    "    if ch : \n",
    "        for i in range(num_chap):\n",
    "            text = str(data['Text'][i])\n",
    "        \n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = remove_stopwords(tokens)\n",
    "            tokens = lemmatizer(tokens)\n",
    "\n",
    "            fDist = FreqDist(tokens)\n",
    "            dictionary_freq = dict(zip(fDist.keys(),fDist.values()))\n",
    "            sorted_dict_freq = sorted(dictionary.items(), key=lambda x: x[1],reverse= True)\n",
    "            \n",
    "        return list(sorted_dict_freq)\n",
    "        \n",
    "    else :\n",
    "        text = \"\"\n",
    "        for i in range(data.shape[0]):\n",
    "            text += str(data['Text'][i])\n",
    "#         text = text.decode('utf-8')\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        tokens = lemmatizer(tokens)\n",
    "\n",
    "        # Frequency\n",
    "        fDist = FreqDist(tokens)\n",
    "        dictionary_freq = dict(zip(fDist.keys(),fDist.values()))\n",
    "        sorted_dict_freq = sorted(dictionary_freq.items(), key=lambda x: x[1],reverse= True)\n",
    "        \n",
    "        print (\"Top \"+str(top_k)+\" Words according to frequency: \")\n",
    "        for i in range(top_k): \n",
    "            print (str(i+1)+\". \"+sorted_dict_freq[i][0],\" : \",sorted_dict_freq[i][1])\n",
    "\n",
    "\n",
    "        # entropy\n",
    "        total = float(len(tokens))\n",
    "        dictionary_entropy = {k: -1 * (float(freq) / total) * np.log(float(freq)/total) for k, freq in dictionary_freq.items()} \n",
    "        sorted_dict_entropy = sorted(dictionary_entropy.items(), key=lambda x: x[1],reverse= True)\n",
    "\n",
    "        print (\"\\n\\nTop \"+str(top_k)+\" Words according to entropy: \")\n",
    "        for i in range(top_k): \n",
    "            print (str(i+1)+\". \"+sorted_dict_entropy[i][0],\" : \",sorted_dict_entropy[i][1])\n",
    "        \n",
    "        #own metric\n",
    "        freq_own = np.zeros(( len(dictionary_freq), num_chap))\n",
    "        index_tokens = {k: i for i,k in enumerate(dictionary_freq)}\n",
    "        \n",
    "        for i, chap in data.iterrows():        \n",
    "            if i>=num_chap : break\n",
    "            freq_dict_chapterwise = chapterwise(chap['Text'])\n",
    "            for k,v in freq_dict_chapterwise.items():\n",
    "                freq_own[index_tokens[k]][i] = v\n",
    "\n",
    "        stand_dev = np.std(freq_own, axis = 1)\n",
    "        std_dict = {k: stand_dev[index_tokens[k]] for k in index_tokens}\n",
    "        sorted_dict_std = sorted(std_dict.items(), key=lambda x: x[1],reverse= True)\n",
    "\n",
    "\n",
    "        print (\"\\n\\nTop \"+str(top_k)+\" Words according to our metric: \")\n",
    "        for i in range(top_k): \n",
    "            print (str(i+1)+\". \"+sorted_dict_std[i][0],\" : \",sorted_dict_std[i][1])\n",
    "            \n",
    "        return list(sorted_dict_freq), list(sorted_dict_entropy), list(sorted_dict_std)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Words according to frequency: \n",
      "1. ,  :  15870\n",
      "2. .  :  11136\n",
      "3. “  :  4244\n",
      "4. ”  :  3796\n",
      "5. ’  :  3381\n",
      "6. Harry  :  3100\n",
      "7. ''  :  2494\n",
      "8. I  :  2373\n",
      "9. ``  :  2369\n",
      "10. say  :  2173\n",
      "11. ?  :  2124\n",
      "12. !  :  1540\n",
      "13. Hermione  :  1206\n",
      "14. He  :  1178\n",
      "15. Ron  :  1154\n",
      "16. 's  :  1031\n",
      "17. The  :  935\n",
      "18. look  :  863\n",
      "19. know  :  787\n",
      "20. –  :  759\n",
      "\n",
      "\n",
      "Top 20 Words according to entropy: \n",
      "1. ,  :  0.22505097982466293\n",
      "2. .  :  0.18179738388285233\n",
      "3. “  :  0.09406600142853855\n",
      "4. ”  :  0.0866996530290858\n",
      "5. ’  :  0.07959057991330709\n",
      "6. Harry  :  0.07460387729180665\n",
      "7. ''  :  0.06330370390489885\n",
      "8. I  :  0.06094679513821453\n",
      "9. ``  :  0.060868253315909414\n",
      "10. say  :  0.056968211731137745\n",
      "11. ?  :  0.055976840374755726\n",
      "12. !  :  0.04358296210309984\n",
      "13. Hermione  :  0.03591521384082345\n",
      "14. He  :  0.03524886493249961\n",
      "15. Ron  :  0.034674505702341596\n",
      "16. 's  :  0.031682057549819544\n",
      "17. The  :  0.029285192001954164\n",
      "18. look  :  0.027448669337695682\n",
      "19. know  :  0.025470561072497504\n",
      "20. –  :  0.024730800744992152\n",
      "\n",
      "\n",
      "Top 20 Words according to our metric: \n",
      "1. ,  :  110.03494899349025\n",
      "2. .  :  105.88956511384869\n",
      "3. ''  :  104.8858903761607\n",
      "4. ``  :  103.04872633856276\n",
      "5. “  :  95.8572375984203\n",
      "6. ’  :  91.23272439207327\n",
      "7. ”  :  82.54629004382934\n",
      "8. Harry  :  37.73075668469956\n",
      "9. 's  :  33.95644268765501\n",
      "10. Ron  :  31.974364731765977\n",
      "11. say  :  31.420375554725627\n",
      "12. ?  :  29.073871431235297\n",
      "13. Kreacher  :  28.2\n",
      "14. I  :  27.34684625327023\n",
      "15. Hermione  :  25.773823930491957\n",
      "16. Hagrid  :  16.57256769483836\n",
      "17. n't  :  16.427111736394806\n",
      "18. Scrimgeour  :  16.05272562526377\n",
      "19. …  :  15.56438241627338\n",
      "20. !  :  15.432433379088343\n"
     ]
    }
   ],
   "source": [
    "a,b,c = book('chapters.csv',top_k = 20, num_chap= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
