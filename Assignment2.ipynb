{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import nltk \n",
    "from time import time\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk import FreqDist,ngrams,word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# import pdftotext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    stop_words = Counter(stopwords.words('english'))\n",
    "    ans = []\n",
    "    for each in data:\n",
    "        if(each not in stop_words.keys()):\n",
    "            ans.append(each)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(tokens):\n",
    "    filtered_document = []\n",
    "    wordnet_pos = ''\n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for i in pos_tokens:\n",
    "        if i[1].startswith('J'):\n",
    "            wordnet_pos = wordnet.ADJ\n",
    "        elif i[1].startswith('V'):\n",
    "            wordnet_pos = wordnet.VERB\n",
    "        elif i[1].startswith('N'):\n",
    "            wordnet_pos = wordnet.NOUN\n",
    "        elif i[1].startswith('R'):\n",
    "            wordnet_pos = wordnet.ADV\n",
    "        else:\n",
    "            wordnet_pos = wordnet.ADJ_SAT\n",
    "        filtered_document.append(lmtzr.lemmatize(i[0],pos = wordnet_pos))\n",
    "    return filtered_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapterwise(text):\n",
    "    tokens = word_tokenize(text[0:2000])\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatizer(tokens)\n",
    "        \n",
    "    fDist = FreqDist(tokens)\n",
    "    dictionary_freq = dict(zip(fDist.keys(),fDist.values()))\n",
    "    sorted_dict_freq = sorted(dictionary_freq.items(), key=lambda x: x[1],reverse= True)\n",
    "    \n",
    "    return dict(sorted_dict_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book(dataset, top_k = 20, num_chap = 10, ch = False):\n",
    "    data = pd.read_csv(dataset)\n",
    "    \n",
    "    if ch : \n",
    "        for i in range(num_chap):\n",
    "            text = data['Text'][i]\n",
    "        \n",
    "            tokens = word_tokenize(text[0:2000])\n",
    "            tokens = remove_stopwords(tokens)\n",
    "            tokens = lemmatizer(tokens)\n",
    "\n",
    "            fDist = FreqDist(tokens)\n",
    "            dictionary_freq = dict(zip(fDist.keys(),fDist.values()))\n",
    "            sorted_dict_freq = sorted(dictionary.items(), key=lambda x: x[1],reverse= True)\n",
    "            \n",
    "        return list(sorted_dict_freq)\n",
    "        \n",
    "    else :\n",
    "        text = \"\"\n",
    "        for i in range(data.shape[0]):\n",
    "            text += data['Text'][i]\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        tokens = lemmatizer(tokens)\n",
    "\n",
    "        # Frequency\n",
    "        fDist = FreqDist(tokens)\n",
    "        dictionary_freq = dict(zip(fDist.keys(),fDist.values()))\n",
    "        sorted_dict_freq = sorted(dictionary_freq.items(), key=lambda x: x[1],reverse= True)\n",
    "        \n",
    "        print \"Top \"+str(top_k)+\" Words according to frequency: \"\n",
    "        for i in range(top_k): \n",
    "            print str(i+1)+\". \"+sorted_dict_freq[i][0],\" : \",sorted_dict_freq[i][1]\n",
    "\n",
    "\n",
    "        # entropy\n",
    "        total = float(len(tokens))\n",
    "        dictionary_entropy = {k: -1 * (float(freq) / total) * np.log(float(freq)/total) for k, freq in dictionary_freq.iteritems()} \n",
    "        sorted_dict_entropy = sorted(dictionary_entropy.items(), key=lambda x: x[1],reverse= True)\n",
    "\n",
    "        print \"\\n\\nTop \"+str(top_k)+\" Words according to entropy: \"\n",
    "        for i in range(top_k): \n",
    "            print str(i+1)+\". \"+sorted_dict_entropy[i][0],\" : \",sorted_dict_entropy[i][1]\n",
    "        \n",
    "        #own metric\n",
    "        freq_own = np.zeros(( len(dictionary_freq), num_chap))\n",
    "        index_tokens = {k: i for i,k in enumerate(dictionary_freq)}\n",
    "        \n",
    "        for i, chap in data.iterrows():        \n",
    "            if i>=num_chap : break\n",
    "            freq_dict_chapterwise = chapterwise(chap['Text'])\n",
    "            for k,v in freq_dict_chapterwise.iteritems():\n",
    "                freq_own[index_tokens[k]][i] = v\n",
    "\n",
    "        stand_dev = np.std(freq_own, axis = 1)\n",
    "        std_dict = {k: stand_dev[index_tokens[k]] for k in index_tokens}\n",
    "        sorted_dict_std = sorted(std_dict.items(), key=lambda x: x[1],reverse= False)\n",
    "\n",
    "\n",
    "        print \"\\n\\nTop \"+str(top_k)+\" Words according to our metric: \"\n",
    "        for i in range(top_k): \n",
    "            print str(i+1)+\". \"+sorted_dict_std[i][0],\" : \",sorted_dict_std[i][1]\n",
    "            \n",
    "        return list(sorted_dict_freq), list(sorted_dict_entropy), list(sorted_dict_std)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-cbbcf664d8b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'questions-data.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chap\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-c64aa7671ae9>\u001b[0m in \u001b[0;36mbook\u001b[0;34m(dataset, top_k, num_chap, ch)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d1e3f3833873>\u001b[0m in \u001b[0;36mlemmatizer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlmtzr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;31m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m         \u001b[0mforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;31m# 2. Return all that are in the database (and check the original too)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mapply_rules\u001b[0;34m(forms)\u001b[0m\n\u001b[1;32m   1823\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubstitutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1825\u001b[0;31m                     if form.endswith(old)]\n\u001b[0m\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "book('questions-data.csv',top_k = 200, num_chap= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
